# -*- coding: utf-8 -*-
"""BERT-FineTuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zUl5UFPeKqQAYRCjdy_kGoI0AdgpuNbU
"""

! pip install tensorflow
! pip install datasets
! pip install transformers # A hugging face library used for models

"""
*   We need a model.
*   And every model expects input in a particular format.
*   When we have input, we need tokenizer to tokenize our input and
    send it.

*   Therefore whenever we're dealing with pre-trained models, there'll be two things that we'll usually deal with - the model itself and the tokenizer ( i.e whatever tokenized version that particular model is expecting in, that is what we'll send )

* We can also use model specific tokenizer or general one.


"""

import tensorflow as tf
from transformers import TFAutoModel, AutoTokenizer, BertModel, BertTokenizer
from datasets import load_dataset

model = TFAutoModel.from_pretrained("bert-base-uncased", use_safetensors=False)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Tokenization examples with sample inputs

inputs = tokenizer(["Hi There", "How are you doing?", "Did you know how she is doing?"], padding=True, truncation=True, return_tensors="tf")
print(inputs)

# If we give return_tensors="pt", we'll get it in PyTorch compatible format.

"""###### With every sentence - At the start, it adds one token called CLS, the the words of sentence and in the end it adds one separator.

###### CLS --WORDS-OF-SENTENCE-- SEPARATOR

BERT is already pre-trained, so we can directly pass inputs to it.
"""

output = model(inputs)
output

"""### Embeddings vs. Context in BERT's Output

It's common to get confused between "embedding" and "context" when discussing models like BERT. Here's a clarification:

*   **Embeddings**: Generally, an embedding is a numerical representation of a piece of data (like a word, phrase, or sentence) in a multi-dimensional space. The idea is that similar items are closer together in this space. Historically, word embeddings (e.g., Word2Vec, GloVe) were *static*; a word like "bank" would have the same embedding regardless of its usage.

*   **Context**: In natural language processing, 'context' refers to the surrounding words, phrases, and sentences that give meaning to a particular word or piece of text. For instance, the meaning of "bank" changes whether it's used in "river bank" or "deposit money in the bank."

#### What BERT Provides: Contextualized Embeddings

BERT doesn't give you 'embedding OR context'; it gives you **contextualized embeddings**. This means that the embeddings BERT generates *already incorporate the context* of the words within the input sequence. Therefore:

When you execute `output = model(inputs)`:

*   **What it does:** This line passes your tokenized inputs (the numerical representations of your text) through the pre-trained BERT model. It's essentially performing a forward pass, where the input data travels through all the layers of the neural network.
*   **What it means:** It's the step where BERT processes your text and computes its internal representations, which are rich with semantic and contextual information.
*   **What it gives:** The `output` variable will contain several pieces of information, most notably:
    *   **`last_hidden_state`**: These are the token-level embeddings. Each word (or sub-word token) in your input receives its own embedding vector. The values of this vector are dynamically determined by the *context* of all other words in the input sentence. So, the embedding for "bank" in "river bank" will be distinct from the embedding for "bank" in "financial bank." These embeddings are rich with contextual information.

    *   **`pooler_output`**: This is a single, aggregated embedding for the *entire input sequence*. It's also highly contextualized and serves as a summary representation of the whole sentence's meaning. It's often used for tasks requiring a single vector for the entire input, like text classification.

In essence, the embeddings produced by BERT are a powerful type of embedding that inherently captures and represents context. The model uses the context to *create* these rich numerical representations.

#### Example: Contextualized Embeddings with Ambiguous Words

Consider these two sentences:

1.  "He sat near a **river bank**."
2.  "She collected money from the **bank**."

When you process these sentences through BERT:

1.  **Embeddings for every word:** You will indeed get unique contextualized embeddings for *every* word (or sub-word token) in both sentences. This includes "He", "sat", "near", "a", "river", and "bank" from the first sentence, and "She", "collected", "money", "from", "the", and "bank" from the second sentence. These token-level embeddings are found in the `last_hidden_state` output.

2.  **Different embeddings for 'bank':** Crucially, the embedding (vector representation) for the word "bank" in the first sentence ("river bank") will be numerically distinct from the embedding for "bank" in the second sentence ("collected money from the bank"). This is because BERT understands the surrounding words and generates an embedding for "bank" that reflects its specific meaning and context within each sentence (one referring to a landform, the other to a financial institution).

What output do we got ?
> This is the contextualized meaning. We basically went through BERT, we gave the inputs and we got the output.

> If we check the shape, it's 3 * 10 * 768.  Earlier the input shape was 3*10.

> Basically, what happens here now is that every token i.e every word is now represented by a 768 dimensional vector. These dimensions vary by every model.

> Next we also get Pooler output : It's 3*768. What it means is for every sentence, they are giving one embedding.

CHATGPT Thread :- https://chatgpt.com/c/69339c6c-2bfc-8323-aa22-2f1df2cff8cf

# TILL NOW WHAT WE'VE DONE IS,
 - We had our BERT model, our inputs and we just passed it through the BERT layers to get contextualized embeddings.

 - Our goal is to have our own dataset and we want to fine tune this model on our new dataset.

 - Dataset that we'll use -> https://huggingface.co/datasets/SetFit/emotion
"""

dataset = load_dataset("SetFit/emotion")

dataset

train_data = dataset["train"]
test_data = dataset["test"]

train_data.shape

train_data[1]

test_data.shape

test_data[0]

# Tokenization function
def tokenize_function(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True)

# Apply tokenization
train_dataset = train_data.map(tokenize_function, batched=True)
test_dataset = test_data.map(tokenize_function, batched=True)

# Convert to PyTorch format
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])



from transformers import AutoModelForSequenceClassification

# Load BERT model (6 output classes)
base_model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=6)

"""### Understanding `TFAutoModel` vs. `AutoModelForSequenceClassification`

When working with the `transformers` library, it's important to understand the subtle but significant differences between loading models for general feature extraction versus specific downstream tasks like classification.

#### BERT's Architecture: An Encoder

First, it's crucial to remember that **BERT (Bidirectional Encoder Representations from Transformers) is an encoder-only model**. Its architecture is based solely on the encoder stack of the original Transformer model. The raw output of a standard Transformer encoder is a sequence of *hidden states* (contextual embeddings for each token), not classification probabilities.

#### `TFAutoModel.from_pretrained("bert-base-uncased")`

*   **Purpose:** This command loads the **base pre-trained BERT model** for **feature extraction**. It provides the core BERT architecture with all its pre-trained layers, but *without any task-specific head* on top.
*   **Output:** When you pass inputs to a model loaded this way, its primary outputs are:
    *   `last_hidden_state`: Contextualized embeddings for each token in the input sequence.
    *   `pooler_output`: A single, aggregated summary embedding for the entire sequence (often derived from the `[CLS]` token).
*   **Use Case:** You would use `TFAutoModel` when your goal is to:
    *   Simply extract rich, contextual embeddings from text for use in another system or as input to a custom model.
    *   Build your *own custom task-specific head* on top of BERT for a novel task that isn't directly covered by existing `AutoModelFor...` classes (e.g., a very specific type of classification layer, or a custom layer for question answering or generation).
    *   Think of it as getting the 'brain' of BERT, ready for you to attach any 'skill' you need.

#### `AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=6)`

*   **Purpose:** This command loads the **base pre-trained BERT model *plus* a classification head** already configured for a sequence classification task. The `num_labels` argument (e.g., `num_labels=6`) specifies how many output classes your classification task has.
*   **How it works:** When you call this, the `transformers` library:
    1.  Loads the pre-trained `bert-base-uncased` encoder model.
    2.  **Attaches a new, randomly initialized classification head** on top of the encoder. This classification head typically consists of one or more simple feed-forward (dense) neural network layers. It takes the pooled output from BERT (usually the hidden state corresponding to the `[CLS]` token) and transforms it into `num_labels` (e.g., 6) output dimensions, representing the raw scores (logits) for each class.
*   **Output:** When you pass inputs to this model, it not only processes them through the BERT layers but also feeds the result into the newly attached classification layer. The final output is logits for each of your `num_labels` classes, which can then be converted to probabilities (e.g., using a softmax function).
*   **Use Case:** You would use this directly when you want to fine-tune BERT for a specific text classification task, such as:
    *   Sentiment analysis (e.g., positive, negative, neutral).
    *   Topic classification (e.g., sports, politics, tech).
    *   Your current task of emotion classification (where `num_labels=6` corresponds to 6 emotion categories).
    *   Think of it as getting the 'brain' of BERT *and* a ready-to-use 'skill' for classification already attached.

**In essence, the key difference is the presence of the task-specific head.** `TFAutoModel` provides the raw feature extractor, while `AutoModelForSequenceClassification` provides the feature extractor *plus* a classification layer, making it immediately suitable for fine-tuning on labeled datasets for classification tasks, without you having to manually construct that final layer.
"""

base_model

model.summary()

"""https://huggingface.co/docs/peft/v0.8.0/en/package_reference/lora"""

# LoRA Configuration

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType

lora_config = LoraConfig(
    r=8,  # Low-rank adaptation dimension
    lora_alpha=32,  # Scaling factor
    lora_dropout=0.05,  # Dropout rate
    target_modules=["query", "value"]  # Apply LoRA to self-attention layers only
)

# Prepare model for LoRA
base_model = prepare_model_for_kbit_training(base_model)

# Convert model into LoRA-enabled model
peft_model = get_peft_model(base_model, lora_config)

# Print trainable parameters
peft_model.print_trainable_parameters()

"""https://huggingface.co/docs/transformers/en/trainer"""

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments( # configuration class that defines how training should happen
    output_dir="./model_checkpoints",  # Where to save model
    num_train_epochs=1,  # Train for 1 epoch
    per_device_train_batch_size=16,  # 16 samples per GPU/CPU
    eval_strategy="epoch",  # Evaluate after every epoch
    save_strategy="epoch",  # Save model after each epoch
    logging_steps=10,  # Log training metrics every 10 steps
    load_best_model_at_end=True,  # Automatically load best checkpoint
    fp16=True,  # Use mixed precision for faster training (if GPU supports it)
    report_to="tensorboard"  # Log training metrics to TensorBoard
)

# A high-level class that automates training, evaluation, and saving models.
# It wraps around your model and dataset, handling:

# Training loops.
# Evaluation during training.
# Model saving & checkpointing.

trainer = Trainer(
    model=peft_model,  # LoRA fine-tuned model
    args=training_args,  # Training settings
    train_dataset=train_dataset,  # Training data
    eval_dataset=test_dataset,  # Test data
    tokenizer=tokenizer  # Tokenizer for processing text,
)

trainer.train()

# inference
from transformers import pipeline, AutoModelForSequenceClassification
from peft import PeftModel

# Load the finetuned model
#         0: "joy",
#         1: "anger",
#         2: "sadness",
#         3: "fear",
#         4: "surprise",
#         5: "neutral"

finetuned_model = AutoModelForSequenceClassification.from_pretrained("./model_checkpoints/checkpoint-1000", num_labels=6)

emotion_classifier = pipeline("text-classification", model=finetuned_model, tokenizer="bert-base-uncased")

# Test on new sentences
print(emotion_classifier("I am so happy today!"))
print(emotion_classifier("I feel terrible and sad."))

